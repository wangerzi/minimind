import streamlit as st
import os
import json
import torch
import glob
import pandas as pd
from datetime import datetime
import warnings
import random
import numpy as np
import sys
import gc

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from transformers import AutoTokenizer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM

warnings.filterwarnings('ignore')

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="MiniMindé¢„è®­ç»ƒæ¨¡åž‹è¯„ä¼°å·¥å…·",
    page_icon="ðŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰åŠ è½½çš„æ¨¡åž‹
if 'current_model' not in st.session_state:
    st.session_state.current_model = None
    st.session_state.current_tokenizer = None
    st.session_state.current_model_path = None

def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def get_available_pretrain_models():
    """èŽ·å–å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡åž‹åˆ—è¡¨"""
    out_dir = "./out"
    if not os.path.exists(out_dir):
        return []
    
    model_files = glob.glob(os.path.join(out_dir, "pretrain_*.pth"))
    return [os.path.basename(f) for f in model_files]

def unload_current_model():
    """å¸è½½å½“å‰æ¨¡åž‹é‡Šæ”¾å†…å­˜"""
    if st.session_state.current_model is not None:
        del st.session_state.current_model
        del st.session_state.current_tokenizer
        st.session_state.current_model = None
        st.session_state.current_tokenizer = None
        st.session_state.current_model_path = None
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def load_model(model_path, hidden_size=512, num_hidden_layers=8, use_moe=False):
    """åŠ è½½æŒ‡å®šçš„æ¨¡åž‹"""
    try:
        # å…ˆå¸è½½å½“å‰æ¨¡åž‹
        unload_current_model()
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained('./model/')
        
        # åˆ›å»ºæ¨¡åž‹é…ç½®
        config = MiniMindConfig(
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            use_moe=use_moe
        )
        
        # åˆ›å»ºæ¨¡åž‹
        model = MiniMindForCausalLM(config)
        
        # åŠ è½½æƒé‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model.load_state_dict(torch.load(model_path, map_location=device), strict=True)
        model = model.eval().to(device)
        
        # ä¿å­˜åˆ°session state
        st.session_state.current_model = model
        st.session_state.current_tokenizer = tokenizer
        st.session_state.current_model_path = model_path
        
        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
        return True, f"æ¨¡åž‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {param_count:.2f}M"
        
    except Exception as e:
        return False, f"æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}"

def generate_text(prompt, max_length=512, temperature=0.85, top_p=0.85):
    """ç”Ÿæˆæ–‡æœ¬"""
    if st.session_state.current_model is None or st.session_state.current_tokenizer is None:
        return "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡åž‹"
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        tokenizer = st.session_state.current_tokenizer
        model = st.session_state.current_model
        
        # é¢„è®­ç»ƒæ¨¡åž‹ç›´æŽ¥ä½¿ç”¨promptï¼Œä¸éœ€è¦chat template
        input_text = tokenizer.bos_token + prompt if tokenizer.bos_token else prompt
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True
        ).to(device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_length,
                num_return_sequences=1,
                do_sample=True,
                attention_mask=inputs["attention_mask"],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                top_p=top_p,
                temperature=temperature
            )
        
        # åªè¿”å›žæ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = tokenizer.decode(
            generated_ids[0][inputs["input_ids"].shape[1]:], 
            skip_special_tokens=True
        )
        return response.strip()
        
    except Exception as e:
        return f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}"

def get_default_prompts():
    """èŽ·å–é»˜è®¤çš„æµ‹è¯•promptåˆ—è¡¨"""
    return [
        'é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŽŸç†',
        'äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½',
        'ä¸‡æœ‰å¼•åŠ›åŽŸç†æ˜¯',
        'ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯',
        'äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­',
        'åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰',
        'æ­å·žå¸‚çš„ç¾Žé£Ÿæœ‰',
        'å½“å®žéªŒæ•°æ®ä¸Žç†è®ºé¢„æµ‹å‡ºçŽ°åå·®æ—¶ï¼Œç§‘å­¦å®¶é¦–å…ˆåº”è¯¥è€ƒè™‘çš„æ˜¯',
        'åœ¨é‡å­åŠ›å­¦ä¸­ï¼Œç²’å­çš„çŠ¶æ€æ˜¯ç”±',
        'é‡‘èžå¸‚åœºçš„é»‘å¤©é¹…äº‹ä»¶å¾€å¾€å…·æœ‰ä¸‰ä¸ªå…±åŒç‰¹å¾ï¼š',
        'åœ¨ç”Ÿç‰©å­¦ä¸­ï¼ŒåŸºå› çš„çªå˜ä¼šå¯¼è‡´',
        'åœ¨ç»æµŽå­¦ä¸­ï¼Œè¾¹é™…æ•ˆç”¨é€’å‡è§„å¾‹è¡¨æ˜Ž',
        'åœ¨ç‰©ç†å­¦ä¸­ï¼Œå…‰çš„æŠ˜å°„çŽ‡ä¸Ž',
        'åœ¨åŒ–å­¦ä¸­ï¼ŒåŒ–å­¦ååº”çš„é€ŸçŽ‡ä¸Ž',
        'åŒºå—é“¾æŠ€æœ¯çš„ä¸å¯ç¯¡æ”¹æ€§ä¸»è¦ä¾èµ–äºŽ',
        # ã€Šé“è¯¡å¼‚ä»™ã€‹ç›¸å…³æµ‹è¯•
        'é“è¯¡å¼‚ä»™çš„ä¸–ç•Œä¸­',
        'æŽç«æ—ºåœ¨çŽ°å®žä¸–ç•Œçš„å¦»å­æ˜¯',
        'è¢„æ™¯æ•™çš„äººä¿®ç‚¼åŠŸæ³•åˆ©ç”¨',
        'é«˜æ™ºåšçš„çœŸå®žèº«ä»½æ˜¯',
        'è¢„æ™¯æ•™ä¿®ç‚¼åŠŸæ³•çš„æ ¸å¿ƒåŽŸç†æ˜¯',
        'é«˜æ™ºåšçš„çœŸå®žèº«ä»½å…¶å®žæ˜¯',
        'åå¿˜é“çš„ä¿®è¡Œæ–¹å¼æ˜¯',
        'è¯¸è‘›æ¸Šä¸ŽæŽç«æ—ºçš„å…³ç³»æœ¬è´¨æ˜¯',
        'å¤§æ¢çš‡å¸å¸å‘½çš„ç§˜å¯†æ˜¯',
        'ç™½çŽ‰äº¬çš„çœŸç›¸æ˜¯',
        'å¿ƒç´ çš„èƒ½åŠ›å…·ä½“è¡¨çŽ°ä¸º',
        'å·´è™ºçš„ä¿¡ä»°è€…ä¼š',
        'å¤©é™ˆå›½çš„åŽ†å²éšè—ç€',
        'æ¸…é£Žè§‚çš„å•ç§€æ‰å®žé™…ä¸Šæ˜¯',
        "ä¿®çœŸå¢ƒç•Œ'åå¿˜'æŒ‡çš„æ˜¯",
        'æŽç«æ—ºçœ‹åˆ°çš„å¹»è§‰ä¸­ç»å¸¸å‡ºçŽ°',
        'å…µå®¶ä¿®å£«çš„ä¿®ç‚¼éœ€è¦',
        'éª°å­åœ¨åå¿˜é“ä¸­è±¡å¾',
        'å¤§å‚©çš„ä»ªå¼å¿…é¡»åŒ…å«',
        'ã€Šå¤§åƒå½•ã€‹è®°è½½çš„ç¦å¿ŒåŒ…æ‹¬',
        'æ³•æ•™ä¿¡å¾’èŽ·å¾—åŠ›é‡çš„æ–¹å¼æ˜¯',
        'ç›‘å¤©å¸çš„åˆ›ç«‹ç›®çš„æ˜¯',
        'å¿ƒæµŠçŽ°è±¡çš„å…¸åž‹ç‰¹å¾æ˜¯',
        'ä¸¹é˜³å­æˆä»™çš„ä»£ä»·æ˜¯',
        'è…Šæœˆåå…«äº‹ä»¶ä¸­å¤±è¸ªçš„',
        'ä¿®çœŸè€…å¯¹æŠ—ç™«ç«çš„æ–¹æ³•æœ‰',
        'å¤§é½çŽ‹æœè¦†ç­çš„çœŸæ­£åŽŸå› æ˜¯',
        'å­£ç¾è¿™ä¸ªåå­—æš—ç¤ºäº†',
        'æ— ç”Ÿè€æ¯çš„é¢„è¨€ä¸­æåˆ°',
        'çŽ„ç‰çš„æ¥åŽ†ä¸Žæœ‰å…³',
        'å¹½éƒ½çš„å…¥å£éšè—åœ¨',
    ]

def save_evaluation_results(results, model_name, note=""):
    """ä¿å­˜è¯„ä¼°ç»“æžœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"eval_{model_name}_{timestamp}.json"
    filepath = os.path.join("eval_logs/pretrains", filename)
    
    eval_data = {
        "timestamp": timestamp,
        "model_name": model_name,
        "model_path": st.session_state.current_model_path,
        "note": note,
        "results": results
    }
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(eval_data, f, ensure_ascii=False, indent=2)
    
    return filepath

def load_evaluation_history():
    """åŠ è½½åŽ†å²è¯„ä¼°ç»“æžœ"""
    history_dir = "eval_logs/pretrains"
    if not os.path.exists(history_dir):
        return []
    
    history_files = glob.glob(os.path.join(history_dir, "eval_*.json"))
    history_data = []
    
    for file_path in sorted(history_files, reverse=True):  # æŒ‰æ—¶é—´å€’åº
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                history_data.append({
                    "filename": os.path.basename(file_path),
                    "timestamp": data.get("timestamp", ""),
                    "model_name": data.get("model_name", ""),
                    "note": data.get("note", ""),
                    "num_tests": len(data.get("results", [])),
                    "data": data,
                    "file_path": file_path
                })
        except Exception as e:
            st.error(f"åŠ è½½åŽ†å²æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    return history_data

def main():
    st.title("ðŸ§  MiniMindé¢„è®­ç»ƒæ¨¡åž‹è¯„ä¼°å·¥å…·")
    st.markdown("---")
    
    # ä¾§è¾¹æ ï¼šæ¨¡åž‹é€‰æ‹©
    with st.sidebar:
        st.header("ðŸ”§ æ¨¡åž‹é…ç½®")
        
        # èŽ·å–å¯ç”¨æ¨¡åž‹
        available_models = get_available_pretrain_models()
        
        if not available_models:
            st.error("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡åž‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿outç›®å½•ä¸‹æœ‰pretrain_*.pthæ–‡ä»¶")
            return
        
        # æ¨¡åž‹é€‰æ‹©
        model_options = ["è¯·é€‰æ‹©æ¨¡åž‹"] + available_models
        selected_model = st.selectbox(
            "é€‰æ‹©é¢„è®­ç»ƒæ¨¡åž‹",
            model_options,
            index=0
        )
        
        # æ¨¡åž‹å‚æ•°é…ç½®
        hidden_size = st.selectbox(
            "Hidden Size",
            [512, 640, 768],
            index=0
        )
        
        num_hidden_layers = st.selectbox(
            "Hidden Layers",
            [8, 16],
            index=0
        )
        
        use_moe = st.checkbox("ä½¿ç”¨MoE")
        
        # å½“é€‰æ‹©æ”¹å˜æ—¶åŠ è½½æ¨¡åž‹
        if selected_model != "è¯·é€‰æ‹©æ¨¡åž‹":
            model_path = f"./out/{selected_model}"
            current_selection = f"{selected_model}_{hidden_size}_{num_hidden_layers}_{use_moe}"
            
            if (st.session_state.current_model_path != model_path or 
                st.session_state.get('current_config') != current_selection):
                
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡åž‹..."):
                    success, message = load_model(model_path, hidden_size, num_hidden_layers, use_moe)
                    if success:
                        st.success(message)
                        st.session_state.current_config = current_selection
                    else:
                        st.error(message)
        
        # æ˜¾ç¤ºå½“å‰æ¨¡åž‹çŠ¶æ€
        if st.session_state.current_model is not None:
            st.success("âœ… æ¨¡åž‹å·²åŠ è½½")
            st.info(f"å½“å‰æ¨¡åž‹: {os.path.basename(st.session_state.current_model_path)}")
        else:
            st.warning("âš ï¸ æœªåŠ è½½æ¨¡åž‹")
    
    # ä¸»ç•Œé¢
    if st.session_state.current_model is None:
        st.warning("è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©å¹¶åŠ è½½æ¨¡åž‹")
        return
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2 = st.tabs(["ðŸ§ª æ¨¡åž‹æµ‹è¯•", "ðŸ“Š åŽ†å²ç»“æžœ"])
    
    with tab1:
        st.header("æ¨¡åž‹æµ‹è¯•")
        
        # æµ‹è¯•è¾“å…¥åŒºåŸŸ
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # é»˜è®¤prompt
            default_prompts = get_default_prompts()
            
            # å°†é»˜è®¤promptsè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            default_text = '\n'.join(default_prompts)
            
            # ç›´æŽ¥ä½¿ç”¨æ–‡æœ¬è¾“å…¥
            custom_input = st.text_area(
                "æµ‹è¯•Promptsï¼ˆæ¯è¡Œä¸€ä¸ªpromptï¼‰",
                value=default_text,
                height=250,
                placeholder="è¯·è¾“å…¥æµ‹è¯•å†…å®¹ï¼Œæ¯è¡Œä¸€ä¸ªprompt",
                help="å¯ä»¥ç›´æŽ¥ç¼–è¾‘é»˜è®¤promptsï¼Œæˆ–è€…æ·»åŠ æ–°çš„æµ‹è¯•å†…å®¹",
                key="prompt_input"
            )
            
            test_prompts = [line.strip() for line in custom_input.split('\n') if line.strip()]
        
        with col2:
            # ç”Ÿæˆå‚æ•°
            st.subheader("ç”Ÿæˆå‚æ•°")
            max_length = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 50, 1024, 256)
            temperature = st.slider("Temperature", 0.1, 2.0, 0.85)
            top_p = st.slider("Top-p", 0.1, 1.0, 0.85)
            
            # æ‰§è¡Œæ¬¡æ•°
            num_runs = st.number_input("æ‰§è¡Œæ¬¡æ•°", min_value=1, max_value=10, value=3)
            
            # éšæœºç§å­è®¾ç½®
            use_fixed_seed = st.checkbox("ä½¿ç”¨å›ºå®šç§å­")
            if use_fixed_seed:
                fixed_seed = st.number_input("éšæœºç§å­", value=2025)
            
            # å¤‡æ³¨ä¿¡æ¯
            st.markdown("---")
            test_note = st.text_area(
                "æµ‹è¯•å¤‡æ³¨",
                placeholder="ä¸ºæœ¬æ¬¡æµ‹è¯•æ·»åŠ å¤‡æ³¨ä¿¡æ¯...",
                height=80,
                help="å¯ä»¥è®°å½•æµ‹è¯•ç›®çš„ã€å‚æ•°è°ƒæ•´åŽŸå› ç­‰ä¿¡æ¯"
            )
        
        # å¼€å§‹æµ‹è¯•æŒ‰é’®
        if st.button("ðŸš€ å¼€å§‹æµ‹è¯•", type="primary", disabled=len(test_prompts) == 0):
            if not test_prompts:
                st.error("è¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥æµ‹è¯•å†…å®¹")
                return
            
            # æ‰§è¡Œæµ‹è¯•
            all_results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            results_container = st.container()
            
            total_tests = len(test_prompts) * num_runs
            current_test = 0
            
            for run_idx in range(num_runs):
                run_results = []
                
                for prompt_idx, prompt in enumerate(test_prompts):
                    current_test += 1
                    progress = current_test / total_tests
                    progress_bar.progress(progress)
                    status_text.text(f"æ‰§è¡Œç¬¬ {run_idx + 1} è½®ï¼Œæµ‹è¯• {prompt_idx + 1}/{len(test_prompts)}: {prompt[:30]}...")
                    
                    # è®¾ç½®éšæœºç§å­
                    if use_fixed_seed:
                        setup_seed(fixed_seed)
                    else:
                        setup_seed(random.randint(0, 2048))
                    
                    # ç”Ÿæˆæ–‡æœ¬
                    response = generate_text(prompt, max_length, temperature, top_p)
                    
                    result = {
                        "run": run_idx + 1,
                        "prompt": prompt,
                        "response": response,
                        "timestamp": datetime.now().isoformat(),
                        "parameters": {
                            "max_length": max_length,
                            "temperature": temperature,
                            "top_p": top_p
                        }
                    }
                    run_results.append(result)
                
                all_results.extend(run_results)
            
            progress_bar.progress(1.0)
            status_text.text("æµ‹è¯•å®Œæˆï¼")
            
            # ä¿å­˜ç»“æžœ
            model_name = os.path.basename(st.session_state.current_model_path).replace('.pth', '')
            saved_file = save_evaluation_results(all_results, model_name, test_note)
            st.success(f"æµ‹è¯•ç»“æžœå·²ä¿å­˜åˆ°: {saved_file}")
            
            # # æ˜¾ç¤ºç»“æžœ
            # with results_container:
            #     st.subheader("æµ‹è¯•ç»“æžœ")
            #     for i, result in enumerate(all_results):
            #         with st.expander(f"ç¬¬{result['run']}è½® - {result['prompt'][:50]}..."):
            #             st.text_area(
            #                 f"è¾“å…¥ (ç¬¬{result['run']}è½®)",
            #                 result['prompt'],
            #                 height=60,
            #                 disabled=True
            #             )
            #             st.text_area(
            #                 "è¾“å‡º",
            #                 result['response'],
            #                 height=120,
            #                 disabled=True
            #             )
    
    with tab2:
        st.header("åŽ†å²æµ‹è¯•ç»“æžœ")
        
        # åŠ è½½åŽ†å²æ•°æ®
        history_data = load_evaluation_history()
        
        if not history_data:
            st.info("æš‚æ— åŽ†å²æµ‹è¯•ç»“æžœ")
            return
        
        # é€‰æ‹©åŽ†å²æ–‡ä»¶
        history_options = [
            f"{item['timestamp']} - {item['model_name']} ({item['num_tests']}ä¸ªæµ‹è¯•)" + 
            (f" - {item['note'][:30]}..." if item['note'] and len(item['note']) > 30 else f" - {item['note']}" if item['note'] else "")
            for item in history_data
        ]
        
        selected_history = st.selectbox(
            "é€‰æ‹©åŽ†å²ç»“æžœ",
            range(len(history_options)),
            format_func=lambda x: history_options[x]
        )
        
        if selected_history is not None:
            selected_data = history_data[selected_history]['data']
            selected_file_path = history_data[selected_history]['file_path']
            
            # æ˜¾ç¤ºæ¦‚è§ˆä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æµ‹è¯•æ—¶é—´", selected_data['timestamp'])
            with col2:
                st.metric("æ¨¡åž‹åç§°", selected_data['model_name'])
            with col3:
                st.metric("æµ‹è¯•æ•°é‡", len(selected_data['results']))
            with col4:
                # è®¡ç®—å¹³å‡å“åº”é•¿åº¦
                avg_length = np.mean([len(r['response']) for r in selected_data['results']])
                st.metric("å¹³å‡å“åº”é•¿åº¦", f"{avg_length:.1f}")
            
            # å¤‡æ³¨ä¿¡æ¯æ˜¾ç¤ºå’Œç¼–è¾‘
            st.markdown("---")
            st.subheader("æµ‹è¯•å¤‡æ³¨")
            
            # ä½¿ç”¨session stateæ¥ç®¡ç†ç¼–è¾‘æ¨¡å¼
            edit_key = f"edit_note_{selected_history}"
            if edit_key not in st.session_state:
                st.session_state[edit_key] = False
            
            current_note = selected_data.get('note', '')
            
            if not st.session_state[edit_key]:
                # æ˜¾ç¤ºæ¨¡å¼
                col_note1, col_note2 = st.columns([4, 1])
                with col_note1:
                    if current_note:
                        st.text_area("", value=current_note, height=80, disabled=True, key=f"note_display_{selected_history}")
                    else:
                        st.text("æš‚æ— å¤‡æ³¨ä¿¡æ¯")
                with col_note2:
                    if st.button("ç¼–è¾‘å¤‡æ³¨", key=f"edit_btn_{selected_history}"):
                        st.session_state[edit_key] = True
                        st.rerun()
            else:
                # ç¼–è¾‘æ¨¡å¼
                new_note = st.text_area(
                    "ç¼–è¾‘å¤‡æ³¨",
                    value=current_note,
                    height=80,
                    key=f"note_edit_{selected_history}"
                )
                col_save1, col_save2, col_save3 = st.columns([1, 1, 3])
                with col_save1:
                    if st.button("ä¿å­˜", key=f"save_btn_{selected_history}"):
                        # æ›´æ–°JSONæ–‡ä»¶
                        selected_data['note'] = new_note
                        with open(selected_file_path, 'w', encoding='utf-8') as f:
                            json.dump(selected_data, f, ensure_ascii=False, indent=2)
                        st.session_state[edit_key] = False
                        st.success("å¤‡æ³¨å·²æ›´æ–°")
                        st.rerun()
                with col_save2:
                    if st.button("å–æ¶ˆ", key=f"cancel_btn_{selected_history}"):
                        st.session_state[edit_key] = False
                        st.rerun()
            
            st.markdown("---")
            
            # åˆ›å»ºç»“æžœè¡¨æ ¼
            results_df = pd.DataFrame([
                {
                    "è½®æ¬¡": r['run'],
                    "è¾“å…¥": r['prompt'][:50] + "..." if len(r['prompt']) > 50 else r['prompt'],
                    "è¾“å‡º": r['response'][:100] + "..." if len(r['response']) > 100 else r['response'],
                    "è¾“å‡ºé•¿åº¦": len(r['response']),
                    "æ—¶é—´": r['timestamp']
                }
                for r in selected_data['results']
            ])
            
            st.dataframe(results_df, use_container_width=True)
            
            # è¯¦ç»†æŸ¥çœ‹
            st.subheader("è¯¦ç»†ç»“æžœ")
            for i, result in enumerate(selected_data['results']):
                with st.expander(f"ç¬¬{result['run']}è½® - {result['prompt'][:50]}..."):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.text_area(
                            "è¾“å…¥",
                            result['prompt'],
                            height=100,
                            disabled=True,
                            key=f"hist_input_{i}"
                        )
                    with col2:
                        st.text_area(
                            "è¾“å‡º", 
                            result['response'],
                            height=100,
                            disabled=True,
                            key=f"hist_output_{i}"
                        )

if __name__ == "__main__":
    main() 