import streamlit as st
import os
import json
import torch
import glob
import pandas as pd
from datetime import datetime
import warnings
import random
import numpy as np
import sys
import gc
from transformers import AutoTokenizer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM


warnings.filterwarnings('ignore')

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="MiniMindé¢„è®­ç»ƒæ¨¡åž‹è¯„ä¼°å·¥å…·",
    page_icon="ðŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰åŠ è½½çš„æ¨¡åž‹
if 'current_model' not in st.session_state:
    st.session_state.current_model = None
    st.session_state.current_tokenizer = None
    st.session_state.current_model_path = None

def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def get_available_pretrain_models():
    """èŽ·å–å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡åž‹åˆ—è¡¨"""
    out_dir = "./out"
    if not os.path.exists(out_dir):
        return []
    
    model_files = glob.glob(os.path.join(out_dir, "pretrain_*.pth"))
    return [os.path.basename(f) for f in model_files]

def unload_current_model():
    """å¸è½½å½“å‰æ¨¡åž‹é‡Šæ”¾å†…å­˜"""
    if st.session_state.current_model is not None:
        del st.session_state.current_model
        del st.session_state.current_tokenizer
        st.session_state.current_model = None
        st.session_state.current_tokenizer = None
        st.session_state.current_model_path = None
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def load_model(model_path, hidden_size=512, num_hidden_layers=8, use_moe=False):
    """åŠ è½½æŒ‡å®šçš„æ¨¡åž‹"""
    try:
        # å…ˆå¸è½½å½“å‰æ¨¡åž‹
        unload_current_model()
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained('./model/')
        
        # åˆ›å»ºæ¨¡åž‹é…ç½®
        config = MiniMindConfig(
            hidden_size=hidden_size,
            num_hidden_layers=num_hidden_layers,
            use_moe=use_moe
        )
        
        # åˆ›å»ºæ¨¡åž‹
        model = MiniMindForCausalLM(config)
        
        # åŠ è½½æƒé‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model.load_state_dict(torch.load(model_path, map_location=device), strict=True)
        model = model.eval().to(device)
        
        # ä¿å­˜åˆ°session state
        st.session_state.current_model = model
        st.session_state.current_tokenizer = tokenizer
        st.session_state.current_model_path = model_path
        
        param_count = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
        return True, f"æ¨¡åž‹åŠ è½½æˆåŠŸï¼å‚æ•°é‡: {param_count:.2f}M"
        
    except Exception as e:
        return False, f"æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}"

def generate_text(prompt, max_length=512, temperature=0.85, top_p=0.85):
    """ç”Ÿæˆæ–‡æœ¬"""
    if st.session_state.current_model is None or st.session_state.current_tokenizer is None:
        return "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡åž‹"
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        tokenizer = st.session_state.current_tokenizer
        model = st.session_state.current_model
        
        # é¢„è®­ç»ƒæ¨¡åž‹ç›´æŽ¥ä½¿ç”¨promptï¼Œä¸éœ€è¦chat template
        input_text = tokenizer.bos_token + prompt if tokenizer.bos_token else prompt
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True
        ).to(device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                inputs["input_ids"],
                max_new_tokens=max_length,
                num_return_sequences=1,
                do_sample=True,
                attention_mask=inputs["attention_mask"],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                top_p=top_p,
                temperature=temperature
            )
        
        # åªè¿”å›žæ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = tokenizer.decode(
            generated_ids[0][inputs["input_ids"].shape[1]:], 
            skip_special_tokens=True
        )
        return response.strip()
        
    except Exception as e:
        return f"ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}"

def get_default_prompts():
    """èŽ·å–é»˜è®¤çš„æµ‹è¯•promptåˆ—è¡¨"""
    return [
        'é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŽŸç†',
        'äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½',
        'ä¸‡æœ‰å¼•åŠ›åŽŸç†æ˜¯',
        'ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯',
        'äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­',
        'åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰',
        'æ­å·žå¸‚çš„ç¾Žé£Ÿæœ‰',
        'å½“å®žéªŒæ•°æ®ä¸Žç†è®ºé¢„æµ‹å‡ºçŽ°åå·®æ—¶ï¼Œç§‘å­¦å®¶é¦–å…ˆåº”è¯¥è€ƒè™‘çš„æ˜¯',
        'åœ¨é‡å­åŠ›å­¦ä¸­ï¼Œç²’å­çš„çŠ¶æ€æ˜¯ç”±',
        'é‡‘èžå¸‚åœºçš„é»‘å¤©é¹…äº‹ä»¶å¾€å¾€å…·æœ‰ä¸‰ä¸ªå…±åŒç‰¹å¾ï¼š',
        'åœ¨ç”Ÿç‰©å­¦ä¸­ï¼ŒåŸºå› çš„çªå˜ä¼šå¯¼è‡´',
        'åœ¨ç»æµŽå­¦ä¸­ï¼Œè¾¹é™…æ•ˆç”¨é€’å‡è§„å¾‹è¡¨æ˜Ž',
        'åœ¨ç‰©ç†å­¦ä¸­ï¼Œå…‰çš„æŠ˜å°„çŽ‡ä¸Ž',
        'åœ¨åŒ–å­¦ä¸­ï¼ŒåŒ–å­¦ååº”çš„é€ŸçŽ‡ä¸Ž',
        'åŒºå—é“¾æŠ€æœ¯çš„ä¸å¯ç¯¡æ”¹æ€§ä¸»è¦ä¾èµ–äºŽ',
        # ã€Šé“è¯¡å¼‚ä»™ã€‹ç›¸å…³æµ‹è¯•
        'é“è¯¡å¼‚ä»™çš„ä¸–ç•Œä¸­',
        'æŽç«æ—ºåœ¨çŽ°å®žä¸–ç•Œçš„å¦»å­æ˜¯',
        'è¢„æ™¯æ•™çš„äººä¿®ç‚¼åŠŸæ³•åˆ©ç”¨',
        'é«˜æ™ºåšçš„çœŸå®žèº«ä»½æ˜¯',
        'è¢„æ™¯æ•™ä¿®ç‚¼åŠŸæ³•çš„æ ¸å¿ƒåŽŸç†æ˜¯',
        'é«˜æ™ºåšçš„çœŸå®žèº«ä»½å…¶å®žæ˜¯',
        'åå¿˜é“çš„ä¿®è¡Œæ–¹å¼æ˜¯',
        'è¯¸è‘›æ¸Šä¸ŽæŽç«æ—ºçš„å…³ç³»æœ¬è´¨æ˜¯',
        'å¤§æ¢çš‡å¸å¸å‘½çš„ç§˜å¯†æ˜¯',
        'ç™½çŽ‰äº¬çš„çœŸç›¸æ˜¯',
        'å¿ƒç´ çš„èƒ½åŠ›å…·ä½“è¡¨çŽ°ä¸º',
        'å·´è™ºçš„ä¿¡ä»°è€…ä¼š',
        'å¤©é™ˆå›½çš„åŽ†å²éšè—ç€',
        'æ¸…é£Žè§‚çš„å•ç§€æ‰å®žé™…ä¸Šæ˜¯',
        "ä¿®çœŸå¢ƒç•Œ'åå¿˜'æŒ‡çš„æ˜¯",
        'æŽç«æ—ºçœ‹åˆ°çš„å¹»è§‰ä¸­ç»å¸¸å‡ºçŽ°',
        'å…µå®¶ä¿®å£«çš„ä¿®ç‚¼éœ€è¦',
        'éª°å­åœ¨åå¿˜é“ä¸­è±¡å¾',
        'å¤§å‚©çš„ä»ªå¼å¿…é¡»åŒ…å«',
        'ã€Šå¤§åƒå½•ã€‹è®°è½½çš„ç¦å¿ŒåŒ…æ‹¬',
        'æ³•æ•™ä¿¡å¾’èŽ·å¾—åŠ›é‡çš„æ–¹å¼æ˜¯',
        'ç›‘å¤©å¸çš„åˆ›ç«‹ç›®çš„æ˜¯',
        'å¿ƒæµŠçŽ°è±¡çš„å…¸åž‹ç‰¹å¾æ˜¯',
        'ä¸¹é˜³å­æˆä»™çš„ä»£ä»·æ˜¯',
        'è…Šæœˆåå…«äº‹ä»¶ä¸­å¤±è¸ªçš„',
        'ä¿®çœŸè€…å¯¹æŠ—ç™«ç«çš„æ–¹æ³•æœ‰',
        'å¤§é½çŽ‹æœè¦†ç­çš„çœŸæ­£åŽŸå› æ˜¯',
        'å­£ç¾è¿™ä¸ªåå­—æš—ç¤ºäº†',
        'æ— ç”Ÿè€æ¯çš„é¢„è¨€ä¸­æåˆ°',
        'çŽ„ç‰çš„æ¥åŽ†ä¸Žæœ‰å…³',
        'å¹½éƒ½çš„å…¥å£éšè—åœ¨',
    ]

def save_evaluation_results(results, model_name, note=""):
    """ä¿å­˜è¯„ä¼°ç»“æžœåˆ°JSONæ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"eval_{model_name}_{timestamp}.json"
    filepath = os.path.join("eval_logs/pretrains", filename)
    
    eval_data = {
        "timestamp": timestamp,
        "model_name": model_name,
        "model_path": st.session_state.current_model_path,
        "note": note,
        "results": results
    }
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(eval_data, f, ensure_ascii=False, indent=2)
    
    return filepath

def load_evaluation_history():
    """åŠ è½½åŽ†å²è¯„ä¼°ç»“æžœ"""
    history_dir = "eval_logs/pretrains"
    if not os.path.exists(history_dir):
        return []
    
    history_files = glob.glob(os.path.join(history_dir, "eval_*.json"))
    history_data = []
    
    for file_path in sorted(history_files, reverse=True):  # æŒ‰æ—¶é—´å€’åº
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                history_data.append({
                    "filename": os.path.basename(file_path),
                    "timestamp": data.get("timestamp", ""),
                    "model_name": data.get("model_name", ""),
                    "note": data.get("note", ""),
                    "num_tests": len(data.get("results", [])),
                    "data": data,
                    "file_path": file_path
                })
        except Exception as e:
            st.error(f"åŠ è½½åŽ†å²æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    return history_data

def main():
    st.title("ðŸ§  MiniMindé¢„è®­ç»ƒæ¨¡åž‹è¯„ä¼°å·¥å…·")
    st.markdown("---")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["ðŸ”§ æ¨¡åž‹ç®¡ç†", "ðŸ§ª æ¨¡åž‹æµ‹è¯•", "ðŸ“Š åŽ†å²ç»“æžœ"])
    
    with tab1:
        st.header("æ¨¡åž‹ç®¡ç†")
        
        # èŽ·å–å¯ç”¨æ¨¡åž‹
        available_models = get_available_pretrain_models()
        
        if not available_models:
            st.error("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡åž‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿outç›®å½•ä¸‹æœ‰pretrain_*.pthæ–‡ä»¶")
        else:
            # æ¨¡åž‹é€‰æ‹©åŒºåŸŸ
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.subheader("é€‰æ‹©æ¨¡åž‹")
                
                # æ¨¡åž‹é€‰æ‹©
                model_options = ["è¯·é€‰æ‹©æ¨¡åž‹"] + available_models
                selected_model = st.selectbox(
                    "é€‰æ‹©é¢„è®­ç»ƒæ¨¡åž‹",
                    model_options,
                    index=0
                )
                
                # æ¨¡åž‹å‚æ•°é…ç½®
                col_param1, col_param2, col_param3 = st.columns(3)
                with col_param1:
                    hidden_size = st.selectbox(
                        "Hidden Size",
                        [512, 640, 768],
                        index=0
                    )
                
                with col_param2:
                    num_hidden_layers = st.selectbox(
                        "Hidden Layers",
                        [8, 16],
                        index=0
                    )
                
                with col_param3:
                    use_moe = st.checkbox("ä½¿ç”¨MoE")
                
                # æ¨¡åž‹æ“ä½œæŒ‰é’®
                col_btn1, col_btn2 = st.columns(2)
                with col_btn1:
                    load_button = st.button("ðŸ”„ åŠ è½½æ¨¡åž‹", type="primary", disabled=(selected_model == "è¯·é€‰æ‹©æ¨¡åž‹"))
                with col_btn2:
                    unload_button = st.button("ðŸ—‘ï¸ å¸è½½æ¨¡åž‹", disabled=(st.session_state.current_model is None))
                
                # å¤„ç†åŠ è½½æ¨¡åž‹
                if load_button and selected_model != "è¯·é€‰æ‹©æ¨¡åž‹":
                    model_path = f"./out/{selected_model}"
                    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡åž‹..."):
                        success, message = load_model(model_path, hidden_size, num_hidden_layers, use_moe)
                        if success:
                            st.success(message)
                            st.rerun()
                        else:
                            st.error(message)
                
                # å¤„ç†å¸è½½æ¨¡åž‹
                if unload_button:
                    unload_current_model()
                    st.success("æ¨¡åž‹å·²å¸è½½")
                    st.rerun()
            
            with col2:
                st.subheader("å½“å‰çŠ¶æ€")
                # æ˜¾ç¤ºå½“å‰æ¨¡åž‹çŠ¶æ€
                if st.session_state.current_model is not None:
                    st.success("âœ… æ¨¡åž‹å·²åŠ è½½")
                    st.info(f"æ¨¡åž‹: {os.path.basename(st.session_state.current_model_path)}")
                    
                    # æ˜¾ç¤ºæ¨¡åž‹å‚æ•°ä¿¡æ¯
                    param_count = sum(p.numel() for p in st.session_state.current_model.parameters() if p.requires_grad) / 1e6
                    st.metric("å‚æ•°é‡", f"{param_count:.2f}M")
                else:
                    st.warning("âš ï¸ æœªåŠ è½½æ¨¡åž‹")
                    st.info("è¯·é€‰æ‹©æ¨¡åž‹å¹¶ç‚¹å‡»åŠ è½½æŒ‰é’®")
    
    with tab2:
        st.header("æ¨¡åž‹æµ‹è¯•")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŠ è½½çš„æ¨¡åž‹
        if st.session_state.current_model is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Žæ¨¡åž‹ç®¡ç†ã€æ ‡ç­¾é¡µä¸­åŠ è½½æ¨¡åž‹")
            st.info("ðŸ’¡ æç¤ºï¼šåˆ‡æ¢åˆ°ã€Žæ¨¡åž‹ç®¡ç†ã€æ ‡ç­¾é¡µï¼Œé€‰æ‹©æ¨¡åž‹å¹¶ç‚¹å‡»ã€ŽåŠ è½½æ¨¡åž‹ã€æŒ‰é’®")
            return
        
        # æµ‹è¯•è¾“å…¥åŒºåŸŸ
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # é»˜è®¤prompt
            default_prompts = get_default_prompts()
            
            # å°†é»˜è®¤promptsè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            default_text = '\n'.join(default_prompts)
            
            # ç›´æŽ¥ä½¿ç”¨æ–‡æœ¬è¾“å…¥
            custom_input = st.text_area(
                "æµ‹è¯•Promptsï¼ˆæ¯è¡Œä¸€ä¸ªpromptï¼‰",
                value=default_text,
                height=250,
                placeholder="è¯·è¾“å…¥æµ‹è¯•å†…å®¹ï¼Œæ¯è¡Œä¸€ä¸ªprompt",
                help="å¯ä»¥ç›´æŽ¥ç¼–è¾‘é»˜è®¤promptsï¼Œæˆ–è€…æ·»åŠ æ–°çš„æµ‹è¯•å†…å®¹",
                key="prompt_input"
            )
            
            test_prompts = [line.strip() for line in custom_input.split('\n') if line.strip()]
        
        with col2:
            # ç”Ÿæˆå‚æ•°
            st.subheader("ç”Ÿæˆå‚æ•°")
            max_length = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 50, 1024, 256)
            temperature = st.slider("Temperature", 0.1, 2.0, 0.85)
            top_p = st.slider("Top-p", 0.1, 1.0, 0.85)
            
            # æ‰§è¡Œæ¬¡æ•°
            num_runs = st.number_input("æ‰§è¡Œæ¬¡æ•°", min_value=1, max_value=10, value=3)
            
            # éšæœºç§å­è®¾ç½®
            use_fixed_seed = st.checkbox("ä½¿ç”¨å›ºå®šç§å­")
            if use_fixed_seed:
                fixed_seed = st.number_input("éšæœºç§å­", value=2025)
            
            # å¤‡æ³¨ä¿¡æ¯
            st.markdown("---")
            test_note = st.text_area(
                "æµ‹è¯•å¤‡æ³¨",
                placeholder="ä¸ºæœ¬æ¬¡æµ‹è¯•æ·»åŠ å¤‡æ³¨ä¿¡æ¯...",
                height=80,
                help="å¯ä»¥è®°å½•æµ‹è¯•ç›®çš„ã€å‚æ•°è°ƒæ•´åŽŸå› ç­‰ä¿¡æ¯"
            )
        
        # å¼€å§‹æµ‹è¯•æŒ‰é’®
        if st.button("ðŸš€ å¼€å§‹æµ‹è¯•", type="primary", disabled=len(test_prompts) == 0):
            if not test_prompts:
                st.error("è¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥æµ‹è¯•å†…å®¹")
                return
            
            # æ‰§è¡Œæµ‹è¯•
            all_results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            results_container = st.container()
            
            total_tests = len(test_prompts) * num_runs
            current_test = 0
            
            for run_idx in range(num_runs):
                run_results = []
                
                for prompt_idx, prompt in enumerate(test_prompts):
                    current_test += 1
                    progress = current_test / total_tests
                    progress_bar.progress(progress)
                    status_text.text(f"æ‰§è¡Œç¬¬ {run_idx + 1} è½®ï¼Œæµ‹è¯• {prompt_idx + 1}/{len(test_prompts)}: {prompt[:30]}...")
                    
                    # è®¾ç½®éšæœºç§å­
                    if use_fixed_seed:
                        setup_seed(fixed_seed)
                    else:
                        setup_seed(random.randint(0, 2048))
                    
                    # ç”Ÿæˆæ–‡æœ¬
                    response = generate_text(prompt, max_length, temperature, top_p)
                    
                    result = {
                        "run": run_idx + 1,
                        "prompt": prompt,
                        "response": response,
                        "timestamp": datetime.now().isoformat(),
                        "parameters": {
                            "max_length": max_length,
                            "temperature": temperature,
                            "top_p": top_p
                        }
                    }
                    run_results.append(result)
                
                all_results.extend(run_results)
            
            progress_bar.progress(1.0)
            status_text.text("æµ‹è¯•å®Œæˆï¼")
            
            # ä¿å­˜ç»“æžœ
            model_name = os.path.basename(st.session_state.current_model_path).replace('.pth', '')
            saved_file = save_evaluation_results(all_results, model_name, test_note)
            st.success(f"æµ‹è¯•ç»“æžœå·²ä¿å­˜åˆ°: {saved_file}")
    
    with tab3:
        st.header("åŽ†å²æµ‹è¯•ç»“æžœ")
        
        # åŠ è½½åŽ†å²æ•°æ®
        history_data = load_evaluation_history()
        
        if not history_data:
            st.info("ðŸ“‹ æš‚æ— åŽ†å²æµ‹è¯•ç»“æžœ")
            st.markdown("ðŸ’¡ **æç¤º**: åœ¨ã€Žæ¨¡åž‹æµ‹è¯•ã€æ ‡ç­¾é¡µä¸­è¿è¡Œæµ‹è¯•åŽï¼Œç»“æžœä¼šæ˜¾ç¤ºåœ¨è¿™é‡Œ")
            return
        
        # åŽ†å²ç»“æžœç®¡ç†åŒºåŸŸ
        col_select, col_delete = st.columns([4, 1])
        
        with col_select:
            # é€‰æ‹©åŽ†å²æ–‡ä»¶
            history_options = [
                f"{item['timestamp']} - {item['model_name']} ({item['num_tests']}ä¸ªæµ‹è¯•)" + 
                (f" - {item['note'][:30]}..." if item['note'] and len(item['note']) > 30 else f" - {item['note']}" if item['note'] else "")
                for item in history_data
            ]
            
            selected_history = st.selectbox(
                "é€‰æ‹©åŽ†å²ç»“æžœ",
                range(len(history_options)),
                format_func=lambda x: history_options[x]
            )
        
        with col_delete:
            st.markdown("ã€€")  # å ä½ç¬¦ï¼Œå¯¹é½é€‰æ‹©æ¡†
            delete_button = st.button("ðŸ—‘ï¸ åˆ é™¤é€‰ä¸­ç»“æžœ", type="secondary")
        
        # å¤„ç†åˆ é™¤æ“ä½œ
        if delete_button and selected_history is not None:
            selected_file_path = history_data[selected_history]['file_path']
            selected_filename = history_data[selected_history]['filename']
            
            # ä½¿ç”¨ç¡®è®¤å¯¹è¯æ¡†
            if f"confirm_delete_{selected_history}" not in st.session_state:
                st.session_state[f"confirm_delete_{selected_history}"] = False
            
            if not st.session_state[f"confirm_delete_{selected_history}"]:
                st.warning(f"âš ï¸ ç¡®è®¤è¦åˆ é™¤ç»“æžœæ–‡ä»¶ã€Ž{selected_filename}ã€å—ï¼Ÿ")
                col_confirm, col_cancel = st.columns(2)
                with col_confirm:
                    if st.button("ç¡®è®¤åˆ é™¤", type="primary", key=f"confirm_{selected_history}"):
                        st.session_state[f"confirm_delete_{selected_history}"] = True
                        st.rerun()
                with col_cancel:
                    if st.button("å–æ¶ˆ", key=f"cancel_{selected_history}"):
                        st.rerun()
            else:
                try:
                    os.remove(selected_file_path)
                    st.success(f"âœ… å·²åˆ é™¤ç»“æžœæ–‡ä»¶: {selected_filename}")
                    # æ¸…ç†session state
                    del st.session_state[f"confirm_delete_{selected_history}"]
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
                    del st.session_state[f"confirm_delete_{selected_history}"]
        
        # æ˜¾ç¤ºé€‰ä¸­çš„åŽ†å²ç»“æžœ
        if selected_history is not None and not delete_button:
            selected_data = history_data[selected_history]['data']
            selected_file_path = history_data[selected_history]['file_path']
            
            st.markdown("---")
            
            # æ˜¾ç¤ºæ¦‚è§ˆä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æµ‹è¯•æ—¶é—´", selected_data['timestamp'])
            with col2:
                st.metric("æ¨¡åž‹åç§°", selected_data['model_name'])
            with col3:
                st.metric("æµ‹è¯•æ•°é‡", len(selected_data['results']))
            with col4:
                # è®¡ç®—å¹³å‡å“åº”é•¿åº¦
                avg_length = np.mean([len(r['response']) for r in selected_data['results']])
                st.metric("å¹³å‡å“åº”é•¿åº¦", f"{avg_length:.1f}")
            
            # å¤‡æ³¨ä¿¡æ¯æ˜¾ç¤ºå’Œç¼–è¾‘
            st.markdown("---")
            st.subheader("æµ‹è¯•å¤‡æ³¨")
            
            # ä½¿ç”¨session stateæ¥ç®¡ç†ç¼–è¾‘æ¨¡å¼
            edit_key = f"edit_note_{selected_history}"
            if edit_key not in st.session_state:
                st.session_state[edit_key] = False
            
            current_note = selected_data.get('note', '')
            
            if not st.session_state[edit_key]:
                # æ˜¾ç¤ºæ¨¡å¼
                col_note1, col_note2 = st.columns([4, 1])
                with col_note1:
                    if current_note:
                        st.text_area("", value=current_note, height=80, disabled=True, key=f"note_display_{selected_history}")
                    else:
                        st.text("æš‚æ— å¤‡æ³¨ä¿¡æ¯")
                with col_note2:
                    if st.button("ç¼–è¾‘å¤‡æ³¨", key=f"edit_btn_{selected_history}"):
                        st.session_state[edit_key] = True
                        st.rerun()
            else:
                # ç¼–è¾‘æ¨¡å¼
                new_note = st.text_area(
                    "ç¼–è¾‘å¤‡æ³¨",
                    value=current_note,
                    height=80,
                    key=f"note_edit_{selected_history}"
                )
                col_save1, col_save2, col_save3 = st.columns([1, 1, 3])
                with col_save1:
                    if st.button("ä¿å­˜", key=f"save_btn_{selected_history}"):
                        # æ›´æ–°JSONæ–‡ä»¶
                        selected_data['note'] = new_note
                        with open(selected_file_path, 'w', encoding='utf-8') as f:
                            json.dump(selected_data, f, ensure_ascii=False, indent=2)
                        st.session_state[edit_key] = False
                        st.success("å¤‡æ³¨å·²æ›´æ–°")
                        st.rerun()
                with col_save2:
                    if st.button("å–æ¶ˆ", key=f"cancel_btn_{selected_history}"):
                        st.session_state[edit_key] = False
                        st.rerun()
            
            st.markdown("---")
            
            # åˆ›å»ºç»“æžœè¡¨æ ¼
            results_df = pd.DataFrame([
                {
                    "è½®æ¬¡": r['run'],
                    "è¾“å…¥": r['prompt'][:50] + "..." if len(r['prompt']) > 50 else r['prompt'],
                    "è¾“å‡º": r['response'][:100] + "..." if len(r['response']) > 100 else r['response'],
                    "è¾“å‡ºé•¿åº¦": len(r['response']),
                    "æ—¶é—´": r['timestamp']
                }
                for r in selected_data['results']
            ])
            
            st.dataframe(results_df, use_container_width=True)
            
            # è¯¦ç»†æŸ¥çœ‹
            st.subheader("è¯¦ç»†ç»“æžœ")
            for i, result in enumerate(selected_data['results']):
                with st.expander(f"ç¬¬{result['run']}è½® - {result['prompt'][:50]}..."):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.text_area(
                            "è¾“å…¥",
                            result['prompt'],
                            height=100,
                            disabled=True,
                            key=f"hist_input_{i}"
                        )
                    with col2:
                        st.text_area(
                            "è¾“å‡º", 
                            result['response'],
                            height=100,
                            disabled=True,
                            key=f"hist_output_{i}"
                        )

if __name__ == "__main__":
    main() 